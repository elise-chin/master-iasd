{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836ec107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import ImageFolder, CIFAR10, MNIST\n",
    "import torchvision.transforms as tfs\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from miscs.pgd import attack_Linf_PGD, attack_FGSM\n",
    "from miscs.loss import *\n",
    "\n",
    "\n",
    "\n",
    "opt = {\n",
    "    \"our_loss\":True,\n",
    "    \"epsilon\":8/255,\n",
    "    \"adv_steps\":10,\n",
    "    \"workers\":5,\n",
    "    \"out_f\":\"MNIST_out\",\n",
    "    \"ngpu\":1,\n",
    "    \"img_width\": 8,\n",
    "    \"root\":\"./data\",\n",
    "    \"dataset\": \"cifar10\",\n",
    "    \"start_width\": 32,\n",
    "    \"model\" :\"resnet_64\",\n",
    "    \"ngf\": 16,\n",
    "    \"ndf\": 16,\n",
    "    \"lr\": 0.0002,\n",
    "    \"batch_size\": 32,\n",
    "    \"nz\": 16,\n",
    "    \"nclass\": 10,\n",
    "    \"starting_epoch\": 0,\n",
    "    \"max_epoch\": 50,\n",
    "    \"iter_d\": 5\n",
    "}\n",
    "\n",
    "def load_models():\n",
    "    \n",
    "    \n",
    "    from gen_models.resnet_32 import ResNetGenerator\n",
    "    from dis_models.resnet_32 import ResNetAC\n",
    "    gen = ResNetGenerator(ch=opt[\"ngf\"], dim_z=opt[\"nz\"], bottom_width=opt[\"start_width\"], n_classes=opt[\"nclass\"])\n",
    "    dis = ResNetAC(ch=opt[\"ndf\"], n_classes=opt[\"nclass\"])\n",
    "    \n",
    "    if opt[\"ngpu\"] > 0:\n",
    "        gen, dis = gen.cuda(), dis.cuda()\n",
    "        gen, dis = torch.nn.DataParallel(gen, device_ids=range(opt[\"ngpu\"])), \\\n",
    "                torch.nn.DataParallel(dis, device_ids=range(opt[\"ngpu\"]))\n",
    "    else:\n",
    "        raise ValueError(\"Must run on gpus, ngpu > 0\")\n",
    "    if opt[\"starting_epoch\"] > 0:\n",
    "        gen.load_state_dict(torch.load(f'./{opt[\"out_f\"]}/gen_epoch_{opt[\"starting_epoch\"]-1}.pth'))\n",
    "        dis.load_state_dict(torch.load(f'./{opt[\"out_f\"]}/dis_epoch_{opt[\"starting_epoch\"]-1}.pth'))\n",
    "    return gen, dis\n",
    "\n",
    "def get_loss():\n",
    "    return loss_nll, loss_nll\n",
    "\n",
    "def make_optimizer(model, beta1=0, beta2=0.9):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=opt[\"lr\"], betas=(beta1, beta2))\n",
    "    return optimizer\n",
    "\n",
    "def make_dataset():\n",
    "    # Small noise is added, following SN-GAN\n",
    "    def noise(x):\n",
    "        return x + torch.FloatTensor(x.size()).uniform_(0, 1.0 / 128)\n",
    "\n",
    "    trans = tfs.Compose([\n",
    "        tfs.RandomCrop(opt[\"img_width\"], padding=4),\n",
    "        tfs.RandomHorizontalFlip(),\n",
    "        tfs.ToTensor(),\n",
    "        tfs.Normalize(mean=[.5], std=[.5]),\n",
    "        tfs.Lambda(noise)])\n",
    "    data = MNIST(root=opt[\"root\"], train=True, download=True, transform=trans)\n",
    "    loader = DataLoader(data, batch_size=opt[\"batch_size\"], shuffle=True, num_workers=opt[\"workers\"])\n",
    "   \n",
    "    return loader\n",
    "\n",
    "def train():\n",
    "    # models\n",
    "    gen, dis = load_models()\n",
    "    # optimizers\n",
    "    opt_g, opt_d = make_optimizer(gen), make_optimizer(dis)\n",
    "    # data\n",
    "    train_loader = make_dataset()\n",
    "    # buffer:\n",
    "    # gaussian noise\n",
    "    z = torch.FloatTensor(opt[\"batch_size\"], opt[\"nz\"]).cuda()\n",
    "    fixed_z = Variable(torch.FloatTensor(8 * 10, opt[\"nz\"]).normal_(0, 1).cuda())\n",
    "    # random label\n",
    "    y_fake = torch.LongTensor(opt[\"batch_size\"]).cuda()\n",
    "    np_y = np.arange(10)\n",
    "    np_y = np.repeat(np_y, 8)\n",
    "    fixed_y_fake = Variable(torch.from_numpy(np_y).cuda())\n",
    "    # fixed label\n",
    "    zeros = Variable(torch.FloatTensor(opt[\"batch_size\"]).fill_(0).cuda())\n",
    "    ones = Variable(torch.FloatTensor(opt[\"batch_size\"]).fill_(1).cuda())\n",
    "    # loss\n",
    "    Ld, Lg = get_loss()\n",
    "    # start training\n",
    "    for epoch in range(opt[\"starting_epoch\"], opt[\"starting_epoch\"] + opt[\"max_epoch\"]):\n",
    "        for count, (x_real, y_real) in enumerate(train_loader):\n",
    "            if count % opt[\"iter_d\"] == 0:\n",
    "                # update generator for every iter_d iterations\n",
    "                gen.zero_grad()\n",
    "                # sample noise\n",
    "                z.normal_(0, 1)\n",
    "                vz = Variable(z)\n",
    "                y_fake.random_(0, to=opt[\"nclass\"])\n",
    "                v_y_fake = Variable(y_fake)\n",
    "                v_x_fake = gen(vz, y=v_y_fake)\n",
    "                v_x_fake_adv = v_x_fake\n",
    "                d_fake_bin, d_fake_multi = dis(v_x_fake_adv)\n",
    "                ones.resize_as_(d_fake_bin.data)\n",
    "                loss_g = Lg(d_fake_bin, ones, d_fake_multi, v_y_fake, lam=0.5)\n",
    "                loss_g.backward()\n",
    "                opt_g.step()\n",
    "                print(f'[{epoch}/{opt[\"max_epoch\"]-1}][{count+1}/{len(train_loader)}][G_ITER] loss_g: {loss_g.item()}')\n",
    "            # update discriminator\n",
    "            dis.zero_grad()\n",
    "            # feed real data\n",
    "            x_real, y_real = x_real.cuda(), y_real.cuda()\n",
    "            v_x_real, v_y_real = Variable(x_real), Variable(y_real)\n",
    "            # find adversarial example\n",
    "            ones.resize_(y_real.size())\n",
    "            v_x_real_adv = attack_Linf_PGD(v_x_real, ones, v_y_real, dis, Ld, opt[\"adv_steps\"], opt[\"epsilon\"])\n",
    "            d_real_bin, d_real_multi = dis(v_x_real_adv)\n",
    "            # accuracy for real images\n",
    "            positive = torch.sum(d_real_bin.data > 0).item()\n",
    "            _, idx = torch.max(d_real_multi.data, dim=1)\n",
    "            correct_real = torch.sum(idx.eq(y_real)).item()\n",
    "            total_real = y_real.numel()\n",
    "            # loss for real images\n",
    "            loss_d_real = Ld(d_real_bin, ones, d_real_multi, v_y_real, lam=0.5)\n",
    "            # feed fake data\n",
    "            z.normal_(0, 1)\n",
    "            y_fake.random_(0, to=opt[\"nclass\"])\n",
    "            vz, v_y_fake = Variable(z), Variable(y_fake)\n",
    "            with torch.no_grad():\n",
    "                v_x_fake = gen(vz, y=v_y_fake)\n",
    "            d_fake_bin, d_fake_multi = dis(v_x_fake.detach())\n",
    "            # accuracy for fake images\n",
    "            negative = torch.sum(d_fake_bin.data > 0).item()\n",
    "            _, idx = torch.max(d_fake_multi.data, dim=1)\n",
    "            correct_fake = torch.sum(idx.eq(y_fake)).item()\n",
    "            total_fake = y_fake.numel()\n",
    "            # loss for fake images\n",
    "            if opt[\"our_loss\"]:\n",
    "                loss_d_fake = Ld(d_fake_bin, zeros, d_fake_multi, v_y_fake, lam=1)\n",
    "            else:\n",
    "                loss_d_fake = Ld(d_fake_bin, zeros, d_fake_multi, v_y_fake, lam=0.5)\n",
    "            loss_d = loss_d_real + loss_d_fake\n",
    "            loss_d.backward()\n",
    "            opt_d.step()\n",
    "            print(f'[{epoch}/{opt[\"max_epoch\"]-1}][{count+1}/{len(train_loader)}][D_ITER] loss_d: {loss_d.item()} acc_r: {positive/total_real}, acc_r@1: {correct_real/total_real}, acc_f: {negative/total_fake}, acc_f@1: {correct_fake/total_fake}')\n",
    "        # generate samples\n",
    "        with torch.no_grad():\n",
    "            fixed_x_fake = gen(fixed_z, y=fixed_y_fake)\n",
    "            fixed_x_fake.data.mul_(0.5).add_(0.5)\n",
    "        x_real.mul_(0.5).add_(0.5)\n",
    "        save_image(fixed_x_fake.data, f'./{opt[\"out_f\"]}/sample_epoch_{epoch}.png', nrow=8)\n",
    "        save_image(x_real, f'./{opt[\"out_f\"]}/real.png')\n",
    "        # save model\n",
    "        torch.save(dis.state_dict(), f'./{opt[\"out_f\"]}/dis_epoch_{epoch}.pth')\n",
    "        torch.save(gen.state_dict(), f'./{opt[\"out_f\"]}/gen_epoch_{epoch}.pth')\n",
    "        # change step size\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            opt[\"lr\"] /= 2\n",
    "            opt_g, opt_d = make_optimizer(gen), make_optimizer(dis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d451280f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:3512: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (68) must match the size of tensor b (64) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_487/3364925475.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_487/3694705529.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0my_fake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mv_y_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0mv_x_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv_y_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0mv_x_fake_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_x_fake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0md_fake_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_fake_multi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_x_fake_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/RobGAN-8a478cf3435387753baee2d3a82d039236cc4fab/gen_models/resnet_32.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, y)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbottom_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbottom_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/RobGAN-8a478cf3435387753baee2d3a82d039236cc4fab/gen_models/resblocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (68) must match the size of tensor b (64) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acabf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in enumerate(trai):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b91fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
